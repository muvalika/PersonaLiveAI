<div align="center">

<img src="assets/header.svg" alt="PersonaLive" width="100%">

<h2>Expressive Portrait Image Animation for Live Streaming</h2>


#### [Zhiyuan Li<sup>1,2,3</sup>](https://huai-chang.github.io/) 路 [Chi-Man Pun<sup>1</sup>](https://cmpun.github.io/)  路 [Chen Fang<sup>2</sup>](http://fangchen.org/) 路 [Jue Wang<sup>2</sup>](https://scholar.google.com/citations?user=Bt4uDWMAAAAJ&hl=en) 路 [Xiaodong Cun<sup>3</sup>](https://vinthony.github.io/academic/) 
<sup>1</sup> University of Macau  &nbsp;&nbsp; <sup>2</sup> [Dzine.ai](https://www.dzine.ai/)  &nbsp;&nbsp; <sup>3</sup> [GVC Lab, Great Bay University](https://gvclab.github.io/)

<img src="assets/demo_1.gif" width="40%"> &nbsp;&nbsp; <img src="assets/demo_2.gif" width="40%">
</div>

##  Updates
- **[2025.12.10]**  Release `inference code`, `config` and `checkpoints`!

## お Framework
<img src="assets/overview.png" alt="Image 1" width="100%">
<p align="justify">
We present PersonaLive, a few-step diffusion-based framework for real-time and streamable portrait animation, which achieves low-latency and stable long-term quality.
</p>


##  Getting Started

<!-- ##  Citation
If you find PersonaLive useful for your research, welcome to  this repo and cite our work using the following BibTeX:
```bibtex

``` -->


## わ Acknowledgement
This project is based on [X-NeMo](https://byteaigc.github.io/X-Portrait2/), [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion) and [RAIN](https://pscgylotti.github.io/pages/RAIN/), thanks to their invaluable contributions.
